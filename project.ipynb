{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1lPoXTFZLBgN"},"outputs":[],"source":["!pip install PyPDF2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LGRsN5BiKaki"},"outputs":[],"source":["import PyPDF2\n","\n","# Open the PDF file\n","with open('/content/RBI.PDF', 'rb') as pdf_file:\n","    reader = PyPDF2.PdfReader(pdf_file)\n","\n","    # Iterate through each page and extract text\n","    pdf_text = \"\" # Initialize an empty string to store the extracted text\n","    for page_num in range(len(reader.pages)):\n","        page = reader.pages[page_num]\n","        page_text = page.extract_text()\n","        pdf_text += page_text # Append the text of each page to pdf_text\n","\n","        # Print each page's text with a separator\n","        print(f\"Page {page_num + 1}:\\n\")\n","        print(page_text)\n","        print(\"\\n\" + \"=\"*40 + \"\\n\")  # Adds a separator between pages\n","\n","# Optionally, save all the extracted text to a file\n","with open('output.txt', 'w', encoding='utf-8') as text_file:\n","    for page_num in range(len(reader.pages)):\n","        page = reader.pages[page_num]\n","        text_file.write(f\"Page {page_num + 1}:\\n\\n\")\n","        text_file.write(page.extract_text())\n","        text_file.write(\"\\n\" + \"=\"*40 + \"\\n\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kNDjGLLvnHcL"},"outputs":[],"source":["!pip install langchain\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QaSPLkCPNT_d"},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# Define a function to chunk non-table text using Langchain\n","def chunk_non_table_text_langchain(text, chunk_size=1000, chunk_overlap=200):\n","    # Create a RecursiveCharacterTextSplitter instance\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=chunk_size,\n","        chunk_overlap=chunk_overlap,\n","        length_function=len\n","    )\n","\n","    # Split the text into lines\n","    lines = text.split('\\n')\n","    non_table_text = []\n","\n","    # Collect non-table lines\n","    for line in lines:\n","        # Skip table-like lines\n","        if \"|\" in line or line.strip().startswith(\":\"):\n","            continue\n","        non_table_text.append(line)\n","\n","    # Join non-table lines back into a single string\n","    non_table_text_str = '\\n'.join(non_table_text)\n","\n","    # Use Langchain's text splitter to create chunks\n","    chunks = text_splitter.split_text(non_table_text_str)\n","\n","    return chunks\n","\n","\n","\n","# Chunk the non-table text\n","chunks = chunk_non_table_text_langchain(pdf_text)\n","\n","# Display the chunks\n","for i, chunk in enumerate(chunks):\n","    print(f\"Chunk {i + 1}:\\n{chunk}\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QTvLiaeGXg-O"},"outputs":[],"source":["!pip install sentence_transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mif46yY_OAn-"},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Load a pre-trained model\n","model = SentenceTransformer('all-MPNet-base-v2')  # You can choose other models available\n","\n","\n","\n","# Create embeddings for each chunk\n","chunk_embeddings = model.encode(chunks, convert_to_numpy=True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-lnOgXMo7DjJ"},"outputs":[],"source":["chunk_embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YUYCHZ60l02X"},"outputs":[],"source":["!pip install chromadb sentence-transformers\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ANwrA1e5l7cs"},"outputs":[],"source":["import chromadb\n","from sentence_transformers import SentenceTransformer\n","\n","# Initialize ChromaDB client\n","client = chromadb.Client()\n","\n","# Create a new collection to store embeddings\n","collection = client.create_collection(\"pdf_chunks_embeddings\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ctskIetBmITz"},"outputs":[],"source":["# Load the SentenceTransformer model\n","model = SentenceTransformer('all-MPNet-base-v2')\n","\n","# Assume chunks contain the text data split into parts from your PDF\n","chunks = chunk_non_table_text_langchain(pdf_text)\n","\n","# Create embeddings for each chunk\n","chunk_embeddings = model.encode(chunks, convert_to_numpy=True)\n","\n","# Store embeddings in ChromaDB\n","for idx, embedding in enumerate(chunk_embeddings):\n","    collection.add(\n","        ids=[str(idx)],  # Use a unique identifier for each chunk, here using index\n","        embeddings=[embedding.tolist()],  # Store the embedding as a list\n","        metadatas=[{\"text\": chunks[idx]}]  # Store the actual chunk text as metadata\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aYb1U7FiQxjN"},"outputs":[],"source":["# Define a query text\n","query = \"what is point 13 IN Agriculture and Climate Change\"\n","\n","# Generate embedding for the query\n","query_embedding = model.encode([query], convert_to_numpy=True)\n","\n","# Search for the most similar chunks in ChromaDB\n","results = collection.query(\n","    query_embeddings=[query_embedding[0].tolist()], # Convert the NumPy array to a list\n","    n_results=3  # Retrieve top 3 most similar chunks\n",")\n","\n","# Display the results\n","for result in results[\"metadatas\"][0]: # Access the first list within \"metadatas\"\n","    print(\"Similar chunk:\", result[\"text\"])\n","    print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bP14FHZoSP2Y"},"outputs":[],"source":["!pip install torch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AXhDslL5y6Z7"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TkgkeWkIzcub"},"outputs":[],"source":["torch.cuda.is_available()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-f7IQQXCzq0A"},"outputs":[],"source":["!pip install flash_attn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-mL8JzqxzCGW"},"outputs":[],"source":["# Load model directly\n","\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","#tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", trust_remote_code=True)\n","tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", trust_remote_code=True)\n","model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\", trust_remote_code=True).half() # Remove .cuda() to avoid using GPU\n","# Check if CUDA is available and move model to GPU if it is\n","if torch.cuda.is_available():\n","    model.to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Cua7aneHzO6e"},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]}],"source":["def generate_prompt(chunk, query):\n","    # Create the prompt template\n","    prompt = f\"Context: {chunk}\\n\\nQuery: {query}\\n\\nResponse:\"\n","    return prompt\n","\n","def get_llm_response(prompt):\n","    inputs = tokenizer(prompt, return_tensors=\"pt\")\n","    outputs = model.generate(**inputs, max_length=32000, num_return_sequences=1)\n","    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","# Create prompt for LLM\n","prompt = generate_prompt(chunk, query) # Use chunk instead of most_relevant_chunk\n","response = get_llm_response(prompt)\n","\n","print(\"LLM Response:\")\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wsVcelhK4gvn"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNmMSyXBZsB65oCDtNE6pWP","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}